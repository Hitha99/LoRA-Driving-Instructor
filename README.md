# ğŸš— LoRA Driving Instructor

A lightweight, instruction-following language model fine-tuned with LoRA to simulate driving-related natural language behavior â€” inspired by self-driving systems like Tesla Autopilot. This project fine-tunes DistilGPT2 on a set of navigation-style commands using HuggingFace Transformers and PEFT for efficient, low-resource adaptation.

---

## âœ¨ Project Highlights

- ğŸ§  Fine-tuned `DistilGPT2` using **LoRA (Low-Rank Adaptation)** to follow navigation-style instructions
- ğŸ’¡ Trained on real-world driving prompts (e.g. â€œTurn left at the lightâ€) using Talk2Car-inspired dataset
- âš™ï¸ Efficient training with only ~1% of model parameters updated using HuggingFace PEFT
- âš¡ Designed to run in Google Colab or low-end local machines (RAM < 16 GB)
- ğŸ“¤ Produces structured, context-aware output for planning agents

---

## ğŸ“¦ Input & Output Examples

**Prompt:**

```
Turn left after the red light and park near the blue building.
```

**Generated Response:**

```
Executing: Turn left after the red light and park near the blue building.
```

---

## ğŸ§  Model Overview

This project uses:
- `DistilGPT2`: A distilled version of GPT2 with 6 layers (~82M params)
- `LoRA`: Low-rank matrix adapters applied to query/value weights
- `PEFT`: HuggingFace library that injects trainable modules into transformer layers

Training is performed only on the LoRA parameters, making the model faster, lighter, and easier to deploy than traditional full fine-tuning.

---

## ğŸ—‚ Project Structure

```
lora-driving-instructor/
â”œâ”€â”€ Dataset/
â”‚   â””â”€â”€ train_commands.json         # Talk2Car-style commands
â”œâ”€â”€ LoRA_Driving_Instructor.py      # Main training script
â”œâ”€â”€ README.md                       # Project overview
â””â”€â”€ .gitignore
```

---

## ğŸ›  Tech Stack

| Tool           | Purpose                              |
|----------------|--------------------------------------|
| PyTorch        | Model backend                        |
| HuggingFace    | Transformers, tokenizers, PEFT       |
| PEFT           | Parameter-Efficient Fine-Tuning (LoRA) |
| Datasets       | HuggingFace Dataset loading + split  |
| Colab / macOS  | Training environment                 |

---

## ğŸš€ Quick Start

### 1. Clone the repository

```bash
git clone git@github.com:Hitha99/lora-driving-instructor.git
cd lora-driving-instructor
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

Or manually:

```bash
pip install torch transformers peft accelerate datasets bitsandbytes
```

### 3. Train the model (LoRA fine-tuning)

```bash
python LoRA_Driving_Instructor.py
```

### 4. Generate output

```bash
python generate.py
```

---

## ğŸ§ª Sample Results

| Prompt                                       | Model Output                                             |
|---------------------------------------------|----------------------------------------------------------|
| Turn left after the red light               | Executing: Turn left after the red light                 |
| Merge onto the highway and keep right       | Executing: Merge onto the highway and keep right         |
| Stop at the crosswalk and wait for the bus  | Executing: Stop at the crosswalk and wait for the bus    |

---

## ğŸ“˜ Related Research & Tools

- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [HuggingFace PEFT Library](https://github.com/huggingface/peft)
- [Talk2Car Dataset](https://github.com/Talk2Car/Talk2Car)
- [Tesla AI Day (YouTube)](https://www.youtube.com/watch?v=j0z4FweCy4M)

---

## ğŸ‘©â€ğŸ’» Author

**Hitha Magadi Vijayanand**  
Graduate Student, Computer Science â€“ Texas A&M University  
ğŸ”— [GitHub](https://github.com/Hitha99)  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/hitha-magadi-vijayanand-68143b1b5/)
